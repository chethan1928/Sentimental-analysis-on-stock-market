async def generate_and_save_reading_comprehension_question_module(user_id, data:dict, db:Session):
    
    if data.get("files"):
        data['files'] = await save_file(data['files'])
    tool = get_tool(data.get("tool_id"), db)
    sources = {
        "youtube": data.get("video_links", []),
        "websites": data.get("website_links", []),
        "files": data.get("files") if data.get("files", None) else [],
        "text": [data.get("text")] if data.get("text", None) else []
    }
    topic = extract_topic_from_inputs(
        describe_requirement=data.get("query"),
        text_input=data.get("text"),
        youtube_links=sources["youtube"],
        files=sources["files"],
        websites=sources["websites"]
    )
    
   
    doc = generate_comprehension(
        topic= topic,
        grade= data.get("grade_level"),
        language= data.get("language"),
        num_questions= data.get("number_of_questions"),
        # question_types= data.get("question_types"),
        question_types = normalize_question_types(data.get("question_types")),
        dok_level=data.get("depth_of_knowledge"),
        bloom_level=data.get("blooms_taxonomy"),
        lexile_level=data.get("lexile_reading_levels"),
        describe_requirement=data.get("query"),
    )
    markdown_data=restructure_to_single_markdown_json(assessment_data= doc, client=client, model_name=settings.AZURE_DEPLOYMENT_NAME)
    #match = re.search(r"\*\*Title:\*\*\s*(.*?)\n", doc)
    #title = match.group(1) if match else ""
    print(markdown_data)
    #final_markdown=save_assessment_to_markdown(assessment_data=markdown_data)
    #print(final_markdown)
    resource = Resource(
        user_id=user_id,
        tool_id = data.get("tool_id"),
        grade = data.get("grade_level"),
        language = data.get("language"),
        course_id = data.get("course_id"),   
        unit_id = data.get("unit_id"),
        lesson_id = data.get("lesson_id"),
        lesson_plan_id = data.get("lesson_plan_id"),  #confirmation needed from arshit
        title = topic,
        query= data.get("query"),
        depth_of_knowledge = data.get("depth_of_knowledge"),
        lexile_read_level = data.get("lexile_reading_levels"),
        blooms_taxonomy = data.get("blooms_taxonomy"),
        number_of_questions= data.get("number_of_questions"),
        # question_types= data.get("question_types"),
        question_types = json.dumps(
            normalize_question_types(data.get("question_types")),
            ensure_ascii=False
        ),
        selected_standard_set = data.get("standard_curriculum_id"),
        selected_standard_course = data.get("standard_course_id"),
        standard = data.get("standard")
    )
    db.add(resource)
    db.commit()
    db.refresh(resource)
    resource_version = ResourceVersion(
        resource_id = resource.id,
        version_number = get_resource_version(resource.id, db),
        content = json.dumps(markdown_data, indent=2, ensure_ascii=False),
        generated_json=doc,
        title=topic,
    )
    db.add(resource_version)
    db.commit()
    db.refresh(resource_version)

    process_source_material(
        db,
        resource.id,
        sources
    )
    return resource.id

async def update_and_generate_reading_comprehension_question_module(user_id, data:dict, db:Session):
    resource:Resource = get_resource(data.get("resource_id"), db)
    if data.get("files"):
        data['files'] = await save_file(data['files'])
    tool = get_tool(data.get("tool_id"), db)
    request_sources = {
        "youtube": data.get("video_links", []),
        "websites": data.get("website_links", []),
        "files": data.get("files") if data.get("files", None) else [],
        "text": [data.get("text")] if data.get("text", None) else []
    }
    process_source_material(
        db,
        data.get("resource_id"),
        request_sources
    )

    sources = get_existing_source_material(data.get("resource_id"), request_sources, db)

    # Fetch existing content so incremental "add more" logic works
    existing_resource_version = db.query(ResourceVersion).filter(
        ResourceVersion.resource_id == data.get("resource_id")
    ).order_by(ResourceVersion.version_number.desc()).first()

    assessment_data = None
    if existing_resource_version and existing_resource_version.generated_json:
        assessment_data = existing_resource_version.generated_json

    topic = extract_topic_from_inputs(
        describe_requirement=data.get("query"),
        text_input=data.get("text"),
        youtube_links=sources["youtube"],
        files=sources["files"],
        websites=sources["websites"]
    )
    doc = generate_comprehension(
        topic= topic,
        grade= data.get("grade_level"),
        language= data.get("language"),
        num_questions= data.get("number_of_questions"),
        # question_types= data.get("question_types"),
        question_types = normalize_question_types(data.get("question_types")),
        dok_level=data.get("depth_of_knowledge"),
        bloom_level=data.get("blooms_taxonomy"),
        lexile_level=data.get("lexile_reading_levels"),
        describe_requirement=data.get("query"),
        assessment_data=assessment_data,
    )
    markdown_data=restructure_to_single_markdown_json(assessment_data= doc, client=client, model_name=settings.AZURE_DEPLOYMENT_NAME)
    print("----------------------",markdown_data)
    #final_markdown=save_assessment_to_markdown(assessment_data=markdown_data)
    #print("----------------------")
    #print(final_markdown)
     
    resource.query= data.get("query")
    resource.user_id=user_id
    resource.tool_id = data.get("tool_id")
    resource.grade = data.get("grade_level")
    resource.language = data.get("language")
    resource.course_id = data.get("course_id")
    resource.unit_id = data.get("unit_id")
    resource.lesson_id = data.get("lesson_id")
    #resource.lesson_plan_id = data.get("lesson_plan_id"),
    resource.depth_of_knowledge = data.get("depth_of_knowledge")
    resource.lexile_read_level = data.get("lexile_reading_levels")
    resource.blooms_taxonomy = data.get("blooms_taxonomy")
    resource.number_of_questions= data.get("number_of_questions")
    # resource.question_types= data.get("question_types"),
    resource.question_types = json.dumps(
        normalize_question_types(data.get("question_types")),
        ensure_ascii=False
    )
    resource.selected_standard_set = data.get("standard_curriculum_id")
    resource.selected_standard_course = data.get("standard_course_id")
    resource.standard = data.get("standard")

    db.commit()
    db.refresh(resource)

    resource_version = ResourceVersion(
        resource_id = resource.id,
        version_number = get_resource_version(resource.id, db),
        content = json.dumps(markdown_data, indent=2, ensure_ascii=False),
        generated_json=doc,
        title=topic,
    )
    db.add(resource_version)
    db.commit()
    db.refresh(resource_version)

    return resource_version

async def enhance_and_update_existing_reading_comprehension_question_module(user_id, data:dict, db:Session):
    #resource:Resource = get_resource(data.get("resource_id"), db)

    #existing_resource_version = db.query(ResourceVersion).filter(ResourceVersion.resource_id == data.get("resource_id")).order_by(ResourceVersion.version_number.desc()).first()
    #existing_prompt=db.query(ResourceVersion).filter(ResourceVersion.resource_id == data.get("resource_id")).order_by(ResourceVersion.version_number.desc()).first().prompt
    resource:Resource = get_resource(data.get("resource_id"), db)
    if data.get("files"):
        data['files'] = await save_file(data['files'])

    request_sources = {
        "youtube": data.get("video_links", []),
        "websites": data.get("website_links", []),
        "files": data.get("files") if data.get("files", None) else [],
        "text": [data.get("text")] if data.get("text", None) else []
    }
    process_source_material(
        db,
        data.get("resource_id"),
        request_sources
    )
    sources = get_existing_source_material(data.get("resource_id"), request_sources, db)
    print("======================")
    print(sources)
    topic = extract_topic_from_inputs(
        describe_requirement=data.get("query"),
        text_input=sources["text"],
        youtube_links=sources["youtube"],
        files=sources["files"],
        websites=sources["websites"]
    )
    print("topic",topic)
    print(data.get("query"))
    print(resource.grade,
        resource.language,
       resource.number_of_questions,
        resource.question_types,
        resource.depth_of_knowledge,
        resource.blooms_taxonomy,
        resource.lexile_read_level)
    #print("----------------------",resource.number_of_questions)
    #print(resource.question_types)

    # Fetch existing content so incremental "add more" logic works
    existing_resource_version = db.query(ResourceVersion).filter(
        ResourceVersion.resource_id == data.get("resource_id")
    ).order_by(ResourceVersion.version_number.desc()).first()

    assessment_data = None
    if existing_resource_version and existing_resource_version.generated_json:
        assessment_data = existing_resource_version.generated_json

    doc = generate_comprehension(
        topic= topic,
        grade= resource.grade,
        language= resource.language,
        num_questions= resource.number_of_questions,
        # question_types= resource.question_types,
        question_types = normalize_question_types(resource.question_types),
        dok_level=resource.depth_of_knowledge,
        bloom_level=resource.blooms_taxonomy,
        lexile_level=resource.lexile_read_level,
        describe_requirement=data.get("query"),
        assessment_data=assessment_data,
    )
    print("doc",doc)
    print("-----------------------------------")
    print(data.get("query"))
    print(resource.grade,
        resource.language,
       resource.number_of_questions,
        resource.question_types,
        resource.depth_of_knowledge,
        resource.blooms_taxonomy,
        resource.lexile_read_level)
    markdown_data=restructure_to_single_markdown_json(assessment_data= doc, client=client, model_name=settings.AZURE_DEPLOYMENT_NAME)
    #final_markdown=save_assessment_to_markdown(assessment_data=markdown_data)
    
     
    db.commit()
    db.refresh(resource)
    resource_version = ResourceVersion(
        title = topic,
        resource_id = resource.id,
        version_number = get_resource_version(resource.id, db),
        prompt = data.get("query"),
        content = json.dumps(markdown_data, indent=2, ensure_ascii=False),
        generated_json=doc,
    )
    db.add(resource_version)
    db.commit()
    db.refresh(resource_version)

    return resource_version
