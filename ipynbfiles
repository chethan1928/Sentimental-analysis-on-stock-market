
        vocab = turn.get("vocabulary") or {}
        for sug in vocab.get("suggestions", []):
            if isinstance(sug, dict):
                better_opts = sug.get("better_options", sug.get("better_word", []))
                # Convert list to tuple to make it hashable, or keep as-is if string
                if isinstance(better_opts, list):
                    better_opts = tuple(better_opts) if better_opts else ()
                vocabulary_suggestions.append({
                    "weak_word": sug.get("word", sug.get("weak_word", "")),
                    "better_options": better_opts

                })







import asyncio
import json
import logging
import os
import re
import shutil
import tempfile
from typing import Optional
import uuid

from fastapi import APIRouter, Form, File, UploadFile, HTTPException, Depends, Request
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator
from openai import AzureOpenAI
from pydub import AudioSegment
from logger_setup import logger
from utils.tts_utils import generate_tts_url
from sqlalchemy import text
import pandas as pd
import io

from models import User, chat_session_db as db, async_session
from utils.agents_utils import load_language_mapping
from utils.user_details import get_current_user
logger = logging.getLogger(__name__)

router = APIRouter()


BOT_NAME = "Merry"


WEIGHTS = {
    "grammar": 0.10,
    "vocabulary": 0.10,
    "pronunciation": 0.10,
    "fluency": 0.10,
    "match": 0.60
}



llm_client = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version=AZURE_OPENAI_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT
)


_whisper_model = WhisperModel("small", compute_type="int8")


GRAMMAR_FIELDS = ["feedback", "filler_feedback", "errors", "word_suggestions", "corrected_sentence", "improved_sentence", "strengths"]
VOCAB_FIELDS = ["feedback", "suggestions"]
PRON_FIELDS = ["feedback", "words_to_practice"]
FLUENCY_FIELDS = ["feedback"]
MATCH_FIELDS = ["feedback", "strengths", "areas_to_improve"]




async def call_llm(prompt: str, mode: str = "chat", timeout: int = 30, model: str = "gpt") -> str:
    """LLM call with timeout"""
    system_prompts = {
        "chat": f"You are {BOT_NAME}, a warm and helpful FAQ practice coach.",
        "analysis": "You are an expert language and interview evaluator. Analyze objectively.",
        "strict_json": "You are a structured evaluator. Respond ONLY in valid JSON. No extra text."
    }
    
    try:
        response = await asyncio.wait_for(
            asyncio.to_thread(
                llm_client.chat.completions.create,
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": system_prompts.get(mode, system_prompts["chat"])},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7 if mode == "chat" else 0.3
            ),
            timeout=timeout
        )
        return response.choices[0].message.content.strip()
    except asyncio.TimeoutError:
        logger.error(f"LLM call timed out after {timeout}s")
        return ""
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        return ""


async def translate_text(text: str, source: str, target: str) -> str:
    """Translate text between languages - supports full language names"""
    if not text or not isinstance(text, str):
        return text if isinstance(text, str) else ""
    
    
    source = source.lower().strip()
    target = target.lower().strip()
    
    languages_data = load_language_mapping()
    if source in languages_data:
        source = languages_data.get(source, source)
    if target in languages_data:
        target = languages_data.get(target, target)
    
    if source == target:
        return text
    
    try:
        translator = GoogleTranslator(source=source, target=target)
        return await asyncio.to_thread(translator.translate, text)
    except Exception as e:
        logger.debug(f"Translation failed: {e}")
        return text


async def make_bilingual(value, source: str, target: str):
    """Convert value to {target, native} structure"""
    if source == target:
        return value
    if isinstance(value, str):
        if not value.strip():
            return {"target": value, "native": value}
        native = await translate_text(value, source, target)
        return {"target": value, "native": native}
    elif isinstance(value, list):
        return await asyncio.gather(*[make_bilingual(item, source, target) for item in value])
    elif isinstance(value, dict):
        keys = list(value.keys())
        translated_values = await asyncio.gather(*[make_bilingual(value[k], source, target) for k in keys])
        return dict(zip(keys, translated_values))
    return value


async def translate_analysis(analysis: dict, source: str, target: str, fields_to_translate: list) -> dict:
    """Translate specified fields in analysis dict"""
    if source == target:
        return analysis
    result = {}
    for k, v in analysis.items():
        if k in fields_to_translate:
            result[k] = await make_bilingual(v, source, target)
        else:
            result[k] = v
    return result


async def transcribe_audio_file(audio_path: str, target_lang: str = "en") -> str:
    """Transcribe audio using Whisper - supports full language names"""
    try:
        
        target_lang = target_lang.lower().strip()
        languages_data = load_language_mapping()
        if target_lang in languages_data:
            target_lang = languages_data.get(target_lang, "en")
        
        segments, _ = await asyncio.to_thread(_whisper_model.transcribe, audio_path, language=target_lang)
        return " ".join([seg.text for seg in segments]).strip()
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        return ""




async def analyze_grammar_llm(user_text: str, model: str = "gpt") -> dict:
    """Grammar analysis with LLM"""
    prompt = f"""Analyze grammar in this SPOKEN text: "{user_text}"

CRITICAL: This is transcribed speech. COMPLETELY IGNORE:
- Punctuation errors, Capitalization errors, Spelling/typo errors

CHECK ONLY FOR THESE GRAMMAR ERRORS:
1. Filler words (um, uh, like, you know, I mean, basically, actually)
2. Wrong prepositions (e.g., "good in" → "good at")
3. Wrong verb tense (e.g., "I goed" → "I went")
4. Subject-verb agreement (e.g., "He don't" → "He doesn't")
5. Missing/wrong articles (e.g., "I am engineer" → "I am an engineer")
6. Word order issues (e.g., "Always I work" → "I always work")

CRITICAL FORMATTING - MUST USE # MARKERS:
- you_said: Copy EXACT sentence, mark WRONG PART with #word#
- should_be: Same sentence with corrections, mark CORRECTED with #word#
- Example: "I #goed# to store" → "I #went# to the store"

Return STRICTLY valid JSON:
{{
  "score": 0-100,
  "filler_words": ["um", "like"],
  "filler_count": 0,
  "filler_feedback": "advice on fillers",
  "errors": [
    {{
      "type": "verb_tense",
      "you_said": "I #goed# to store",
      "should_be": "I #went# to the store",
      "wrong_word": "goed",
      "correct_word": "went",
      "explanation": "Go is irregular - past tense is went"
    }}
  ],
  "word_suggestions": [{{"you_used": "good", "use_instead": "excellent", "original_sentence": "It was #good#", "improved_sentence": "It was #excellent#"}}],
  "corrected_sentence": "sentence with grammar fixed",
  "improved_sentence": "polished version",
  "strengths": ["what they did well"],
  "feedback": "2-3 sentences of feedback"
}}

RULES:
- ALWAYS use # on both sides of wrong/correct words
- Empty arrays [] if no issues
"""
    try:
        raw = await call_llm(prompt, mode="strict_json", model=model)
        match = re.search(r'\{[\s\S]*\}', raw)
        if match:
            data = json.loads(match.group())
            data.setdefault("score", 75)
            data.setdefault("filler_words", [])
            data.setdefault("filler_count", len(data.get("filler_words", [])))
            data.setdefault("errors", [])
            data.setdefault("word_suggestions", [])
            data.setdefault("strengths", [])
            return data
    except:
        pass
    return {"score": 75, "filler_words": [], "filler_count": 0, "errors": [], "word_suggestions": [], "strengths": [], "feedback": "Grammar analyzed."}


async def analyze_vocab_llm(user_text: str, model: str = "gpt") -> dict:
    """Vocabulary analysis with LLM"""
    prompt = f"""Analyze vocabulary in this spoken text: "{user_text}"

Evaluate CEFR level distribution (A1=basic, A2=elementary, B1=intermediate, B2=upper-intermediate, C1=advanced, C2=proficient).

IDENTIFY WEAK/BASIC WORDS TO REPLACE:
- Look for: good, bad, nice, thing, get, make, very, really, a lot, stuff
- Suggest professional/interview-appropriate alternatives

CRITICAL FORMATTING - MUST USE # MARKERS:
- original_sentence: Copy EXACT sentence containing weak word, mark with #word#
- improved_sentence: Same sentence with better word, mark with #better_word#

Return STRICTLY valid JSON:
{{
  "score": 0-100,
  "overall_level": "A1/A2/B1/B2/C1/C2",
  "total_words": <word count>,
  "cefr_distribution": {{
    "A1": {{"percentage": 20, "words": ["I", "is"]}},
    "A2": {{"percentage": 30, "words": ["good", "nice"]}},
    "B1": {{"percentage": 40, "words": ["actually", "however"]}},
    "B2": {{"percentage": 10, "words": ["sophisticated"]}},
    "C1": {{"percentage": 0, "words": []}},
    "C2": {{"percentage": 0, "words": []}}
  }},
  "professional_words_used": ["excellent", "demonstrate"],
  "suggestions": [
    {{
      "word": "good",
      "current_level": "A2",
      "better_word": "excellent",
      "suggested_level": "B1",
      "original_sentence": "I had a #good# experience",
      "improved_sentence": "I had an #excellent# experience"
    }}
  ],
  "feedback": "2-3 sentences about vocabulary range"
}}

RULES:
- ALWAYS mark words with #word# (single hash on both sides)
- suggestions array MUST NOT be empty if weak words exist
"""
    try:
        raw = await call_llm(prompt, mode="strict_json", model=model)
        match = re.search(r'\{[\s\S]*\}', raw)
        if match:
            data = json.loads(match.group())
            data.setdefault("score", 75)
            data.setdefault("suggestions", [])
            data.setdefault("professional_words_used", [])
            return data
    except:
        pass
    return {"score": 75, "total_words": len(user_text.split()), "suggestions": [], "professional_words_used": [], "feedback": "Vocabulary analyzed."}


async def analyze_pronunciation_llm(audio_path: str, target_lang: str = "en", model: str = "gpt") -> dict:
    """Pronunciation analysis using Whisper confidence + LLM for detailed feedback - matches Fluent API"""
    try:
        # Language mapping
        lang_code = target_lang.lower().strip()
        languages_data = load_language_mapping()
        if lang_code in languages_data:
            lang_code = languages_data.get(lang_code, "en")
        
        segments, _ = await asyncio.to_thread(_whisper_model.transcribe, audio_path, word_timestamps=True, language=lang_code)
        words_data = []
        transcription = ""
        for seg in segments:
            transcription += seg.text + " "
            if seg.words:
                for w in seg.words:
                    words_data.append({"word": w.word.strip().lower(), "confidence": w.probability})
        
        transcription = transcription.strip()
        
        if not words_data:
            return {"score": 0, "transcription": "", "feedback": "No speech detected", "word_analysis": [], "words_to_practice": []}
        
        accuracy = int((sum(w["confidence"] for w in words_data) / len(words_data)) * 100)
        
        # Build per-word pronunciation data
        CONFIDENCE_THRESHOLD = 0.70
        mispronounced_words = []
        well_pronounced = []
        all_words_pronunciation = []
        
        for wd in words_data:
            word = wd["word"].strip(".,!?")
            if len(word) < 2:
                continue
            
            pronunciation_percentage = round(wd["confidence"] * 100, 1)
            
            if pronunciation_percentage >= 90:
                status = "excellent"
            elif pronunciation_percentage >= 70:
                status = "good"
            elif pronunciation_percentage >= 50:
                status = "needs_improvement"
            else:
                status = "poor"
            
            all_words_pronunciation.append({
                "word": word,
                "pronunciation_percentage": pronunciation_percentage,
                "status": status
            })
            
            if wd["confidence"] < CONFIDENCE_THRESHOLD:
                mispronounced_words.append({
                    "word": word,
                    "confidence": round(wd["confidence"] * 100, 1),
                    "issue": "unclear pronunciation" if wd["confidence"] < 0.5 else "slight pronunciation issue"
                })
            else:
                well_pronounced.append(word)
        
        # LLM enhancement for detailed feedback with phonetic guides
        llm_prompt = f"""You are a pronunciation coach.

TRANSCRIPTION: "{transcription}"

PER-WORD PRONUNCIATION SCORES (confidence-based):
{all_words_pronunciation}

MISPRONOUNCED WORDS (low confidence):
{mispronounced_words if mispronounced_words else "None - all words were clear!"}

WELL PRONOUNCED WORDS: {well_pronounced[:10]}

OVERALL ACCURACY: {accuracy}%

For each word, provide pronunciation guidance with phonetic/syllable guides.

Return STRICTLY valid JSON:
{{
    "word_analysis": [
        {{
            "word": "the word",
            "pronunciation_match": 85.5,
            "rating": "excellent/good/needs_improvement/poor",
            "phonetic_guide": "how to pronounce: ex-AM-ple",
            "improvement_tip": "specific tip if needed, or null if pronunciation is good"
        }}
    ],
    "words_to_practice": [
        {{
            "word": "the word",
            "how_to_say": "syllable breakdown with stress: ex-AM-ple",
            "tip": "specific tip to pronounce this word better"
        }}
    ],
    "well_pronounced_words": ["list", "of", "good", "words"],
    "overall_feedback": "2-3 sentences about pronunciation quality",
    "top_improvement_tip": "one key tip to improve pronunciation"
}}"""
        
        try:
            raw = await call_llm(llm_prompt, mode="strict_json", model=model, timeout=20)
            match = re.search(r'\{[\s\S]*\}', raw)
            if match:
                llm_data = json.loads(match.group())
            else:
                raise ValueError("No JSON")
        except:
            # Fallback if LLM fails
            llm_data = {
                "word_analysis": all_words_pronunciation,
                "words_to_practice": [{"word": w["word"], "how_to_say": w["word"].upper(), "tip": "Speak slowly and clearly"} for w in mispronounced_words[:5]],
                "well_pronounced_words": well_pronounced[:10],
                "overall_feedback": f"Pronunciation accuracy: {accuracy}%",
                "top_improvement_tip": "Focus on clarity and speak at a steady pace"
            }
        
        return {
            "score": accuracy,
            "transcription": transcription,
            "word_pronunciation_scores": [{
                "word": w["word"],
                "score": round(w["confidence"] * 100, 1),
                "status": "good" if w["confidence"] >= 0.7 else "needs_improvement"
            } for w in words_data],
            "word_analysis": llm_data.get("word_analysis", all_words_pronunciation),
            "words_to_practice": llm_data.get("words_to_practice", [{"word": w["word"], "how_to_say": w["word"], "tip": "Focus on clarity"} for w in mispronounced_words[:5]]),
            "well_pronounced_words": llm_data.get("well_pronounced_words", well_pronounced[:10]),
            "feedback": llm_data.get("overall_feedback", f"Pronunciation accuracy: {accuracy}%."),
            "top_improvement_tip": llm_data.get("top_improvement_tip", "Focus on clarity"),
            "mispronounced_count": len(mispronounced_words)
        }
    except Exception as e:
        logger.error(f"Pronunciation analysis error: {e}")
        return {"score": 70, "transcription": "", "feedback": "Pronunciation analyzed.", "word_analysis": [], "words_to_practice": []}


def analyze_fluency(user_text: str, duration: float) -> dict:
    """Fluency analysis based on WPM"""
    word_count = len(re.findall(r"\b\w+\b", user_text or ""))
    wpm = int((word_count / duration) * 60) if duration > 0 else 100
    
    if wpm < 80:
        score = max(40, 60 - (80 - wpm))
        status = "too_slow"
    elif wpm < 110:
        score = 70 + (wpm - 80)
        status = "slow"
    elif wpm <= 160:
        score = 90 + min(10, (wpm - 110) // 5)
        status = "normal"
    elif wpm <= 180:
        score = 85
        status = "fast"
    else:
        score = max(60, 85 - (wpm - 180) // 2)
        status = "too_fast"
    
    return {
        "score": min(100, score),
        "wpm": wpm,
        "speed_status": status,
        "duration_seconds": round(duration, 1),
        "feedback": f"Speaking speed: {status.replace('_', ' ')} ({wpm} WPM)."
    }


async def analyze_match_llm(question: str, user_answer: str, sample_answer: str, model: str = "gpt") -> dict:
    """Match analysis - evaluate answer quality (sample answer is internal reference only)"""
    
    reference = f"\nReference for evaluation (internal): {sample_answer[:200]}" if sample_answer else ""
    
    prompt = f"""Evaluate this answer quality:

Question: {question}
User's Answer: {user_answer}
{reference}

Analyze: relevance, clarity, structure, completeness.
Generate an IMPROVED version based on the USER'S ANSWER (enhance their style, don't replace).

Return STRICTLY valid JSON:
{{
  "score": 0-100,
  "is_relevant": true/false,
  "strengths": ["what user did well"],
  "areas_to_improve": ["specific improvements needed"],
  "feedback": "constructive feedback on answer quality",
  "improved_answer": "enhanced version of user's answer maintaining their style"
}}"""
    
    try:
        raw = await call_llm(prompt, mode="strict_json", model=model, timeout=40)
        match = re.search(r'\{[\s\S]*\}', raw)
        if match:
            return json.loads(match.group())
    except:
        pass
    return {"score": 70, "feedback": "Answer evaluated.", "is_relevant": True, "improved_answer": user_answer}


async def compare_attempts(prev: dict, current: dict, question: str, model: str = "gpt") -> dict:
    """Compare two attempts with detailed LLM analysis"""
    prev_score = prev.get("overall_score", 0) or 0
    current_score = current.get("overall_score", 0) or 0
    diff = round(current_score - prev_score, 2)
    
    
    prev_scores = prev.get("scores", {})
    current_scores = current.get("scores", {})
    
    
    field_diffs = {}
    for field in ["grammar", "vocabulary", "pronunciation", "fluency", "match"]:
        p = prev_scores.get(field, 0) or 0
        c = current_scores.get(field, 0) or 0
        field_diffs[field] = {"prev": p, "current": c, "diff": round(c - p, 1)}
    
    if diff > 10:
        trend = "significantly_improved"
    elif diff > 0:
        trend = "improved"
    elif diff < -10:
        trend = "declined"
    elif diff < 0:
        trend = "slightly_declined"
    else:
        trend = "no_change"
    
    prompt = f"""Compare these two attempts at the same question with DETAILED per-field analysis:

QUESTION: {question}

PREVIOUS ATTEMPT:
- Overall Score: {prev_score}%
- Grammar: {field_diffs['grammar']['prev']}%
- Vocabulary: {field_diffs['vocabulary']['prev']}%
- Pronunciation: {field_diffs['pronunciation']['prev']}%
- Fluency: {field_diffs['fluency']['prev']}%
- Match: {field_diffs['match']['prev']}%
- Answer: "{prev.get('transcription', '')[:150]}"

CURRENT ATTEMPT:
- Overall Score: {current_score}% ({'+' if diff > 0 else ''}{diff}%)
- Grammar: {field_diffs['grammar']['current']}% ({'+' if field_diffs['grammar']['diff'] > 0 else ''}{field_diffs['grammar']['diff']}%)
- Vocabulary: {field_diffs['vocabulary']['current']}% ({'+' if field_diffs['vocabulary']['diff'] > 0 else ''}{field_diffs['vocabulary']['diff']}%)
- Pronunciation: {field_diffs['pronunciation']['current']}% ({'+' if field_diffs['pronunciation']['diff'] > 0 else ''}{field_diffs['pronunciation']['diff']}%)
- Fluency: {field_diffs['fluency']['current']}% ({'+' if field_diffs['fluency']['diff'] > 0 else ''}{field_diffs['fluency']['diff']}%)
- Match: {field_diffs['match']['current']}% ({'+' if field_diffs['match']['diff'] > 0 else ''}{field_diffs['match']['diff']}%)
- Answer: "{current.get('transcription', '')[:150]}"

Provide detailed per-category analysis.

Return STRICTLY valid JSON:
{{
    "overall_summary": "3-4 sentences comparing the attempts",
    "grammar_analysis": {{"improved": true/false, "feedback": "specific grammar feedback"}},
    "vocabulary_analysis": {{"improved": true/false, "feedback": "specific vocabulary feedback"}},
    "pronunciation_analysis": {{"improved": true/false, "feedback": "specific pronunciation feedback"}},
    "fluency_analysis": {{"improved": true/false, "feedback": "specific fluency feedback"}},
    "match_analysis": {{"improved": true/false, "feedback": "specific answer match feedback"}},
    "biggest_improvement": "most improved area",
    "focus_area": "area needing most work",
    "encouragement": "motivational message",
    "next_tip": "one specific tip"
}}"""
    
    try:
        raw = await call_llm(prompt, mode="strict_json", model=model, timeout=30)
        match = re.search(r'\{[\s\S]*\}', raw)
        if match:
            llm_data = json.loads(match.group())
        else:
            raise ValueError("No JSON")
    except:
        llm_data = {
            "overall_summary": f"Score changed from {prev_score}% to {current_score}%.",
            "grammar_analysis": {"improved": field_diffs['grammar']['diff'] > 0, "feedback": "Grammar comparison unavailable."},
            "vocabulary_analysis": {"improved": field_diffs['vocabulary']['diff'] > 0, "feedback": "Vocabulary comparison unavailable."},
            "pronunciation_analysis": {"improved": field_diffs['pronunciation']['diff'] > 0, "feedback": "Pronunciation comparison unavailable."},
            "fluency_analysis": {"improved": field_diffs['fluency']['diff'] > 0, "feedback": "Fluency comparison unavailable."},
            "match_analysis": {"improved": field_diffs['match']['diff'] > 0, "feedback": "Match comparison unavailable."},
            "encouragement": "Keep practicing!"
        }
    
    return {
        "previous_score": prev_score,
        "current_score": current_score,
        "score_change": diff,
        "trend": trend,
        "per_field_comparison": field_diffs,
        **llm_data
    }


async def generate_session_summary(session_data: dict, model: str = "gpt") -> dict:
    """Generate comprehensive session summary with all attempts"""
    attempts = session_data.get("attempts", [])
    question = session_data.get("question", {})
    user_name = session_data.get("name", "Learner")
    
    if not attempts:
        return {"message": "No attempts to summarize."}
    
    
    scores = [a.get("overall_score", 0) for a in attempts]
    best_score = max(scores)
    latest_score = scores[-1]
    
    attempt_summaries = []
    for i, a in enumerate(attempts, 1):
        attempt_summaries.append(f"Attempt {i}: {a.get('overall_score', 0)}%")
    
    prompt = f"""Generate a session summary:

User: {user_name}
Question: {question.get('question', '')[:100]}
Attempts: {len(attempts)}
Score progression: {' -> '.join(map(str, scores))}
Best score: {best_score}%

Return STRICTLY valid JSON:
{{
    "overall_assessment": "3-4 sentences summarizing the session",
    "score_progression": {json.dumps(scores)},
    "best_score": {best_score},
    "total_attempts": {len(attempts)},
    "strengths": ["top 3 strengths"],
    "improvement_areas": ["top 3 areas to work on"],
    "recommendations": ["2-3 actionable next steps"],
    "encouragement": "motivational closing"
}}"""
    
    try:
        raw = await call_llm(prompt, mode="strict_json", model=model, timeout=30)
        match = re.search(r'\{[\s\S]*\}', raw)
        if match:
            return json.loads(match.group())
    except:
        pass
    
    return {
        "overall_assessment": f"{user_name}, you completed {len(attempts)} attempt(s) with a best score of {best_score}%.",
        "score_progression": scores,
        "best_score": best_score,
        "total_attempts": len(attempts),
        "strengths": [],
        "improvement_areas": [],
        "recommendations": ["Keep practicing", "Focus on clarity"],
        "encouragement": "Great effort!"
    }




@router.post("/practice_faq")
async def practice_faq(
    request: Request,
    name: str = Form(default="Learner"),
    target_lang: str = Form(default="en"),
    native_lang: str = Form(default="hi"),
    question_id: Optional[int] = Form(default=None),
    session_id: Optional[str] = Form(default=None),
    audio_file: Optional[UploadFile] = File(default=None),
    text_input: Optional[str] = Form(default=None),
    action: Optional[str] = Form(default=None),
    model: Optional[str] = Form(default="gpt"),
    current_user: User = Depends(get_current_user),
):
    
    native = native_lang
    user_id = current_user.id
    if action == "end":
        
        if session_id:
            session_data = await db.get_user_session(session_id)
            if not session_data:
                raise HTTPException(status_code=404, detail="Session not found")
        elif user_id and question_id:
            
            user_sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
            session_data = None
            for s in user_sessions:
                if s.get("question_id") == question_id:
                    session_id = s["session_id"]
                    session_data = await db.get_user_session(session_id)
                    break
            if not session_data:
                raise HTTPException(status_code=404, detail=f"No session found for user {user_id} and question {question_id}")
        else:
            raise HTTPException(status_code=400, detail="Provide session_id or user_id+question_id to end session")
        
        native = session_data.get("native_lang", native_lang)
        
        
        summary = await generate_session_summary(session_data, model=model)
        summary_bilingual = await make_bilingual(summary, "en", native)
        
        
        attempts = session_data.get("attempts", [])
        all_attempts_bilingual = []
        
        for i, attempt in enumerate(attempts, 1):
            all_attempts_bilingual.append({
                "attempt_number": i,
                "score": attempt.get("overall_score", 0),
                "transcription": attempt.get("transcription", ""),
                "analysis": attempt.get("analysis", {}),
                "improved_answer": attempt.get("improved_answer", {}),
                "comparison": attempt.get("comparison")  
            })
        
        
        session_data["status"] = "paused"
        session_data["final_feedback"] = summary_bilingual  
        await db.update_session(session_id, session_data, overall_score=summary.get("best_score"))
        
        return {
            "action": "session_paused",
            "session_id": session_id,
            "target_lang": target_lang,
            "native_lang": native,
            "session_summary": summary_bilingual,
            "all_attempts": all_attempts_bilingual
        }
    
    
    if not question_id and not session_id:
        raise HTTPException(status_code=400, detail="Provide question_id for new session or session_id for existing")
    
    if not audio_file and not text_input:
        raise HTTPException(status_code=400, detail="Provide audio_file or text_input")
    
    
    if session_id:
        session_data = await db.get_user_session(session_id)
        if not session_data:
            raise HTTPException(status_code=404, detail="Session not found")
        
        cur_q = session_data["question"]
        native = session_data.get("native_lang", native_lang)
    else:
        
        existing_session_id = None
        if user_id and question_id:
            user_sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
            for s in user_sessions:
                if s.get("question_id") == question_id:
                    existing_session_id = s["session_id"]
                    break
        
        if existing_session_id:
            
            session_id = existing_session_id
            session_data = await db.get_user_session(session_id)
            cur_q = session_data["question"]
            native = session_data.get("native_lang", native_lang)
        else:
            
            async with async_session() as sess:
                result = await sess.execute(
                    text("SELECT id, category, question, sample_answer FROM faq_questions WHERE id = :qid"),
                    {"qid": question_id}
                )
                row = result.fetchone()
            
            if not row:
                raise HTTPException(status_code=404, detail=f"Question ID {question_id} not found")
            
            cur_q = {"id": row[0], "category": row[1], "question": row[2], "sample_answer": row[3]}
            session_id = str(uuid.uuid4())
            session_data = {
                "name": name,
                "question": cur_q,
                "target_lang": target_lang,
                "native_lang": native_lang,
                "attempts": [],
                "status": "active"
            }
            await db.create_session(session_id, "faq", session_data, user_id=user_id, user_name=name)
    
    
    temp_path = None
    duration = 0
    is_audio = audio_file is not None
    target_lang = session_data.get("target_lang", "en")
    
    if audio_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            shutil.copyfileobj(audio_file.file, tmp)
            temp_path = tmp.name
        
        try:
            audio = await asyncio.to_thread(AudioSegment.from_file, temp_path)
            duration = len(audio) / 1000.0
            user_text = await transcribe_audio_file(temp_path, target_lang)
        except Exception as e:
            if temp_path and os.path.exists(temp_path):
                os.unlink(temp_path)
            return {"action": "error", "message": f"Audio processing failed: {str(e)}"}
    else:
        user_text = text_input.strip() if text_input else ""
        duration = len(user_text.split()) / 2.0
    
    if not user_text:
        if temp_path and os.path.exists(temp_path):
            os.unlink(temp_path)
        return {"action": "error", "message": "No input detected"}
    
    
    grammar_task = analyze_grammar_llm(user_text, model=model)
    vocab_task = analyze_vocab_llm(user_text, model=model)
    match_task = analyze_match_llm(cur_q["question"], user_text, cur_q.get("sample_answer"), model=model)
    
    if is_audio:
        pron_task = analyze_pronunciation_llm(temp_path, target_lang, model=model)
        grammar_data, vocab_data, match_data, pron_data = await asyncio.gather(
            grammar_task, vocab_task, match_task, pron_task
        )
        fluency_data = analyze_fluency(user_text, duration)
    else:
        grammar_data, vocab_data, match_data = await asyncio.gather(
            grammar_task, vocab_task, match_task
        )
        pron_data = {"score": 0, "feedback": "N/A for text input"}
        fluency_data = {"score": 0, "wpm": 0, "feedback": "N/A for text input"}
    
    
    if temp_path and os.path.exists(temp_path):
        os.unlink(temp_path)
    
    
    scores = {
        "grammar": grammar_data.get("score", 75),
        "vocabulary": vocab_data.get("score", 75),
        "pronunciation": pron_data.get("score", 0) if is_audio else 0,
        "fluency": fluency_data.get("score", 0) if is_audio else 0,
        "match": match_data.get("score", 70)
    }
    
    
    final_score = int(
        scores["grammar"] * WEIGHTS["grammar"] +
        scores["vocabulary"] * WEIGHTS["vocabulary"] +
        scores["pronunciation"] * WEIGHTS["pronunciation"] +
        scores["fluency"] * WEIGHTS["fluency"] +
        scores["match"] * WEIGHTS["match"]
    )
    
    
    
    text_na_msg = await make_bilingual("N/A for text input", "en", native) if not is_audio else None
    
    analysis = {
        "grammar": await translate_analysis(grammar_data, "en", native, GRAMMAR_FIELDS),
        "vocabulary": await translate_analysis(vocab_data, "en", native, VOCAB_FIELDS),
        "pronunciation": await translate_analysis(pron_data, "en", native, PRON_FIELDS) if is_audio else {"score": 0, "feedback": text_na_msg},
        "fluency": await translate_analysis(fluency_data, "en", native, FLUENCY_FIELDS) if is_audio else {"score": 0, "feedback": text_na_msg},
        "match": await translate_analysis(match_data, "en", native, MATCH_FIELDS)
    }
    
    
    improved_answer = await make_bilingual(match_data.get("improved_answer", user_text), "en", native)
    
    
    attempt_entry = {
        "transcription": user_text,
        "duration": duration,
        "is_audio": is_audio,
        "overall_score": final_score,
        "scores": scores,
        "analysis": analysis,
        "improved_answer": improved_answer,
        "filler_count": grammar_data.get("filler_count", 0)
    }
    
    
    attempts = session_data.get("attempts", [])
    prev_attempt = attempts[-1] if attempts else None
    
    
    comparison = None
    if prev_attempt:
        comparison_raw = await compare_attempts(prev_attempt, attempt_entry, cur_q["question"], model=model)
        comparison = await make_bilingual(comparison_raw, "en", native)
        
        attempt_entry["comparison"] = comparison
    
    
    attempts.append(attempt_entry)
    session_data["attempts"] = attempts
    session_data["status"] = "active"
    
    
    await db.update_session(session_id, session_data, overall_score=final_score)
    
    # Generate TTS audio URL for the question (uses Indian English voice for FAQ)
    question_audio_url = await generate_tts_url(request, cur_q["question"], target_lang, api_type="faq")
    
    response = {
        "action": "answer_evaluated",
        "session_id": session_id,
        "target_lang": target_lang,
        "native_lang": native,
        "question_id": cur_q["id"],
        "question": await make_bilingual(cur_q["question"], "en", native),
        "attempt_number": len(attempts),
        "transcription": user_text,
        "overall_score": final_score,
        "scores": scores,
        "analysis": analysis,
        "improved_answer": improved_answer,
        "audio_url": question_audio_url
    }
    
    if comparison:
        response["comparison"] = comparison
    
    return response


@router.get("/faq_questions")
async def get_faq_questions(
    category: Optional[str] = None,
    current_user: User = Depends(get_current_user),
):
    """Get FAQ questions. If user_id provided, includes attempt counts for that user."""
    async with async_session() as sess:
        query = "SELECT id, category, question, min_time, max_time, sample_answer FROM faq_questions"
        if category:
            result = await sess.execute(
                text(query + " WHERE LOWER(category) LIKE :cat ORDER BY category, id"),
                {"cat": f"%{category.lower()}%"}
            )
        else:
            result = await sess.execute(text(query + " ORDER BY category, id"))
        rows = result.fetchall()

    questions = []

    user_id = current_user.id if current_user else None
    user_sessions_data = None
    if user_id:
        user_sessions_data = await db.get_sessions_by_user_id(user_id, session_type="faq")

    for r in rows:
        q = {
            "id": r[0],
            "category": r[1],
            "question": r[2],
            "min_time": r[3],
            "max_time": r[4],
            "sample_answer": r[5],
            "attempt_count": 0,  
            "sessions": []       
        }


        if user_id and user_sessions_data:

            question_sessions = [
                {"session_id": s["session_id"], "attempt_count": s["attempt_count"]}
                for s in user_sessions_data if s.get("question_id") == r[0]
            ]
            q["sessions"] = question_sessions

            q["attempt_count"] = sum(s["attempt_count"] for s in question_sessions)

        questions.append(q)

    return {"total": len(questions), "questions": questions}


@router.get("/faq_sessions")
async def get_faq_sessions(current_user: User = Depends(get_current_user)):
    """Get all FAQ sessions for a user"""
    user_id = current_user.id
    sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
    return {
        "user_id": user_id,
        "total_sessions": len(sessions),
        "sessions": sessions
    }


@router.get("/faq_topics")
async def get_faq_topics():
    """Get available FAQ categories"""
    async with async_session() as sess:
        result = await sess.execute(
            text("SELECT DISTINCT category, COUNT(*) FROM faq_questions GROUP BY category")
        )
        return {"topics": [{"name": r[0], "count": r[1]} for r in result.fetchall()]}


@router.get("/faq_feedback")
async def get_faq_feedback(
    session_id: Optional[str] = None,
    question_id: Optional[int] = None,
    current_user: User = Depends(get_current_user)
):
    """Get structured feedback for a FAQ session (all attempts + overall feedback).
    Can use either session_id OR user_id+question_id."""
    
    user_id = current_user.id if current_user else None
    if not session_id and user_id and question_id:
        user_sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
        for s in user_sessions:
            if s.get("question_id") == question_id:
                session_id = s["session_id"]
                break
    
    if not session_id:
        raise HTTPException(status_code=400, detail="Provide session_id or user_id+question_id")
    
    feedback = await db.get_session_feedback(session_id)
    if not feedback:
        raise HTTPException(status_code=404, detail="Session not found")
    
    grammar_errors = []
    vocabulary_suggestions = []
    pronunciation_issues = []
    
    turn_feedback = feedback.get("turn_feedback", [])
    
    for turn in turn_feedback:
        # Grammar errors
        grammar = turn.get("grammar") or {}
        for err in grammar.get("errors", []):
            if isinstance(err, dict):
                grammar_errors.append({
                    "wrong": err.get("you_said", err.get("wrong_word", "")),
                    "correct": err.get("should_be", err.get("correct_word", ""))
                })
        
        # Vocabulary suggestions
        vocab = turn.get("vocabulary") or {}
        for sug in vocab.get("suggestions", []):
            if isinstance(sug, dict):
                vocabulary_suggestions.append({
                    "weak_word": sug.get("word", sug.get("weak_word", "")),
                    "better_options": sug.get("better_options", sug.get("better_word", []))
                })
        
        # Pronunciation issues
        pron = turn.get("pronunciation") or {}
        if isinstance(pron, dict):
            for word in pron.get("words_to_practice", []):
                if isinstance(word, dict):
                    pronunciation_issues.append({"word": word.get("word", ""), "issue": word.get("issue", "needs practice")})
                elif isinstance(word, str):
                    pronunciation_issues.append({"word": word, "issue": "needs practice"})
    
    # Remove duplicates - convert dicts to strings for hashability (bilingual values are dicts)
    def make_hashable(val):
        if isinstance(val, dict):
            target = val.get("target")
            if isinstance(target, dict):
                target = target.get("target")
            if isinstance(target, (str, int, float)):
                return str(target)
            word = val.get("word")
            if isinstance(word, (str, int, float)):
                return str(word)
            try:
                return json.dumps(val, sort_keys=True)
            except Exception:
                return str(val)
        if isinstance(val, list):
            return tuple(make_hashable(v) for v in val)
        return val
    
    seen = set()
    unique_grammar = []
    for g in grammar_errors:
        wrong_val = make_hashable(g.get("wrong"))
        correct_val = make_hashable(g.get("correct"))
        if wrong_val and (wrong_val, correct_val) not in seen:
            seen.add((wrong_val, correct_val))
            unique_grammar.append(g)
    
    seen = set()
    unique_vocab = []
    for v in vocabulary_suggestions:
        weak_word = make_hashable(v.get("weak_word"))
        if weak_word and weak_word not in seen:
            seen.add(weak_word)
            unique_vocab.append(v)
    
    seen = set()
    unique_pron = []
    for p in pronunciation_issues:
        word = make_hashable(p.get("word"))
        if word and word not in seen:
            seen.add(word)
            unique_pron.append(p)
    
    # Add summary to feedback response
    feedback["summary"] = {
        "grammar": {"total_errors": len(unique_grammar), "errors": unique_grammar},
        "vocabulary": {"total_suggestions": len(unique_vocab), "suggestions": unique_vocab},
        "pronunciation": {"total_issues": len(unique_pron), "issues": unique_pron}
    }
    
    # Aggregate vocab CEFR words and WPM per turn
    wpm_per_turn = []
    vocab_overall = {
        "A1": {"count": 0, "words": []},
        "A2": {"count": 0, "words": []},
        "B1": {"count": 0, "words": []},
        "B2": {"count": 0, "words": []},
        "C1": {"count": 0, "words": []},
        "C2": {"count": 0, "words": []}
    }
    
    for i, turn in enumerate(turn_feedback, 1):
        # Track WPM per turn
        fluency_data = turn.get("fluency") or {}
        turn_wpm = fluency_data.get("wpm", 0) if fluency_data else 0
        wpm_per_turn.append({"turn": i, "wpm": turn_wpm})
        
        # Aggregate CEFR vocabulary words
        vocab_data = turn.get("vocabulary") or {}
        cefr_dist = vocab_data.get("cefr_distribution", {}) if vocab_data else {}
        for level in ["A1", "A2", "B1", "B2", "C1", "C2"]:
            level_data = cefr_dist.get(level, {})
            if isinstance(level_data, dict):
                words = level_data.get("words", [])
                if isinstance(words, list):
                    vocab_overall[level]["words"].extend(words)
                    # Count will be calculated after deduplication below
    
    # Deduplicate vocab words and calculate percentages
    # Helper to get string from word (handles dict {"target": ..., "native": ...} or plain string)
    def word_to_str(w):
        if isinstance(w, dict):
            return w.get("target", str(w))
        return str(w) if w else ""
    
    # Deduplicate using string representation
    for level in vocab_overall:
        seen_words = set()
        unique_words = []
        for w in vocab_overall[level]["words"]:
            w_str = word_to_str(w)
            if w_str and w_str not in seen_words:
                seen_words.add(w_str)
                unique_words.append(w)
        vocab_overall[level]["words"] = unique_words
        vocab_overall[level]["count"] = len(unique_words)
    
    total_vocab_words = sum(vocab_overall[level]["count"] for level in vocab_overall)
    for level in vocab_overall:
        vocab_overall[level]["percentage"] = round((vocab_overall[level]["count"] / total_vocab_words * 100), 1) if total_vocab_words > 0 else 0
    
    feedback["wpm_per_turn"] = wpm_per_turn
    feedback["vocab_overall"] = vocab_overall
    
    return feedback


@router.post("/faq/upload")
async def upload_faq_excel(
    file: UploadFile = File(...),
    replace_all: bool = Form(default=False)
):
    """Upload FAQ questions from Excel/CSV"""
    
    content = await file.read()
    try:
        if file.filename.endswith('.xlsx') or file.filename.endswith('.xls'):
            df = pd.read_excel(io.BytesIO(content))
        elif file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(content))
        else:
            return {"status": "error", "message": "Unsupported format"}
        
        df.columns = df.columns.str.strip().str.lower()
        if len(df.columns) >= 3:
            df.rename(columns={df.columns[2]: 'sample_answer'}, inplace=True)
        
        required = ['category', 'question']
        if not all(col in df.columns for col in required):
            return {"status": "error", "message": f"Missing columns: {required}"}
        
        faq_items = []
        for _, row in df.iterrows():
            faq_items.append({
                "category": str(row['category']).strip(),
                "question": str(row['question']).strip(),
                "sample_answer": str(row.get('sample_answer', '')).strip() if pd.notna(row.get('sample_answer')) else None,
                "min_time": int(row.get('min_time', 30)) if pd.notna(row.get('min_time')) else 30,
                "max_time": int(row.get('max_time', 90)) if pd.notna(row.get('max_time')) else 90
            })
        
        async with async_session() as sess:
            if replace_all:
                await sess.execute(text("DELETE FROM faq_questions"))
            
            for item in faq_items:
                await sess.execute(
                    text("INSERT INTO faq_questions (category, question, sample_answer, min_time, max_time) VALUES (:c, :q, :s, :mi, :ma)"),
                    {"c": item['category'], "q": item['question'], "s": item['sample_answer'], "mi": item['min_time'], "ma": item['max_time']}
                )
            await sess.commit()
        
        return {"status": "success", "count": len(faq_items)}
    except Exception as e:
        return {"status": "error", "message": str(e)}
@router.get("/faq_question_sessinons/{question_id}")

async def get_faq_question_sessions(
    question_id: int,
    current_user: User = Depends(get_current_user)
):  
    """
    Get all session IDs and details for a specific FAQ question for the current user.
    Returns session history to use with /faq_feedback endpoint.
    """
    user_id = current_user.id
    
    # Get all user's FAQ sessions
    all_sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
    
    # Filter for this specific question
    question_sessions = []
    for session in all_sessions:
        if session.get("question_id") == question_id:
            question_sessions.append({
                "session_id": session["session_id"],
                "attempt_count": session.get("attempt_count", 0),
                "overall_score": session.get("overall_score"),
                "status": session.get("status", "unknown"),
                "created_at": session.get("created_at"),
                "updated_at": session.get("updated_at")
            })
    
    if not question_sessions:
        # Get question details even if no sessions exist
        async with async_session() as sess:
            result = await sess.execute(
                text("SELECT id, category, question FROM faq_questions WHERE id = :qid"),
                {"qid": question_id}
            )
            row = result.fetchone()
        
        if not row:
            raise HTTPException(status_code=404, detail=f"Question ID {question_id} not found")
        
        return {
            "user_id": user_id,
            "question_id": question_id,
            "question": row[2],
            "category": row[1],
            "total_sessions": 0,
            "sessions": [],
            "message": "No practice sessions found for this question yet"
        }
    
    # Get question details
    async with async_session() as sess:
        result = await sess.execute(
            text("SELECT id, category, question FROM faq_questions WHERE id = :qid"),
            {"qid": question_id}
        )
        row = result.fetchone()
    
    return {
        "user_id": user_id,
        "question_id": question_id,
        "question": row[2] if row else None,
        "category": row[1] if row else None,
        "total_sessions": len(question_sessions),
        "sessions": sorted(question_sessions, key=lambda x: x.get("created_at", ""), reverse=True)
    }


@router.get("/faq_feedback_v2")
async def get_faq_feedback(
    session_id: Optional[str] = None,
    question_id: Optional[int] = None,
    current_user: User = Depends(get_current_user)
):
    """
    Get structured feedback for a FAQ session (all attempts + overall feedback).
    Can use either session_id OR user_id+question_id.
    
    AUTO-GENERATES final feedback if session was never ended with action='end'.
    """
    
    user_id = current_user.id if current_user else None
    if not session_id and user_id and question_id:
        user_sessions = await db.get_sessions_by_user_id(user_id, session_type="faq")
        for s in user_sessions:
            if s.get("question_id") == question_id:
                session_id = s["session_id"]
                break
    
    if not session_id:
        raise HTTPException(status_code=400, detail="Provide session_id or user_id+question_id")
    
    feedback = await db.get_session_feedback(session_id)
    if not feedback:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # Get full session data for auto-generating final feedback if needed
    session_data = await db.get_user_session(session_id)
    attempts = session_data.get("attempts", []) if session_data else []
    
    # AUTO-GENERATE FINAL FEEDBACK if it doesn't exist
    if not feedback.get("final_feedback") and attempts:
        logger.info(f"Auto-generating final feedback for session {session_id}")
        summary = await generate_session_summary(session_data, model="gpt")
        native = session_data.get("native_lang", "en")
        final_feedback = await make_bilingual(summary, "en", native)
        
        # Save it back to session
        session_data["final_feedback"] = final_feedback
        session_data["status"] = "paused"  # Mark as paused if it wasn't already
        await db.update_session(session_id, session_data, overall_score=summary.get("best_score"))
        
        # Add to response
        feedback["final_feedback"] = final_feedback
    
    grammar_errors = []
    vocabulary_suggestions = []
    pronunciation_issues = []
    
    turn_feedback = feedback.get("turn_feedback", [])
    
    for turn in turn_feedback:
        # Grammar errors
        grammar = turn.get("grammar") or {}
        for err in grammar.get("errors", []):
            if isinstance(err, dict):
                grammar_errors.append({
                    "wrong": err.get("you_said", err.get("wrong_word", "")),
                    "correct": err.get("should_be", err.get("correct_word", ""))
                })
        
        # Vocabulary suggestions
        vocab = turn.get("vocabulary") or {}
        for sug in vocab.get("suggestions", []):
            if isinstance(sug, dict):
                vocabulary_suggestions.append({
                    "weak_word": sug.get("word", sug.get("weak_word", "")),
                    "better_options": sug.get("better_options", sug.get("better_word", []))
                })
        
        # Pronunciation issues
        pron = turn.get("pronunciation") or {}
        if isinstance(pron, dict):
            for word in pron.get("words_to_practice", []):
                if isinstance(word, dict):
                    pronunciation_issues.append({"word": word.get("word", ""), "issue": word.get("issue", "needs practice")})
                elif isinstance(word, str):
                    pronunciation_issues.append({"word": word, "issue": "needs practice"})
    
    # Remove duplicates - convert dicts to strings for hashability (bilingual values are dicts)
    def make_hashable(val):
        if isinstance(val, dict):
            target = val.get("target")
            if isinstance(target, dict):
                target = target.get("target")
            if isinstance(target, (str, int, float)):
                return str(target)
            word = val.get("word")
            if isinstance(word, (str, int, float)):
                return str(word)
            try:
                return json.dumps(val, sort_keys=True)
            except Exception:
                return str(val)
        if isinstance(val, list):
            return tuple(make_hashable(v) for v in val)
        return val
    
    seen = set()
    unique_grammar = []
    for g in grammar_errors:
        wrong_val = make_hashable(g.get("wrong"))
        correct_val = make_hashable(g.get("correct"))
        if wrong_val and (wrong_val, correct_val) not in seen:
            seen.add((wrong_val, correct_val))
            unique_grammar.append(g)
    
    seen = set()
    unique_vocab = []
    for v in vocabulary_suggestions:
        weak_word = make_hashable(v.get("weak_word"))
        if weak_word and weak_word not in seen:
            seen.add(weak_word)
            unique_vocab.append(v)
    
    seen = set()
    unique_pron = []
    for p in pronunciation_issues:
        word = make_hashable(p.get("word"))
        if word and word not in seen:
            seen.add(word)
            unique_pron.append(p)
    
    # Add summary to feedback response
    feedback["summary"] = {
        "grammar": {"total_errors": len(unique_grammar), "errors": unique_grammar},
        "vocabulary": {"total_suggestions": len(unique_vocab), "suggestions": unique_vocab},
        "pronunciation": {"total_issues": len(unique_pron), "issues": unique_pron}
    }
    
    # Aggregate vocab CEFR words and WPM per turn
    wpm_per_turn = []
    vocab_overall = {
        "A1": {"count": 0, "words": []},
        "A2": {"count": 0, "words": []},
        "B1": {"count": 0, "words": []},
        "B2": {"count": 0, "words": []},
        "C1": {"count": 0, "words": []},
        "C2": {"count": 0, "words": []}
    }
    
    for i, turn in enumerate(turn_feedback, 1):
        # Track WPM per turn
        fluency_data = turn.get("fluency") or {}
        turn_wpm = fluency_data.get("wpm", 0) if fluency_data else 0
        wpm_per_turn.append({"turn": i, "wpm": turn_wpm})
        
        # Aggregate CEFR vocabulary words
        vocab_data = turn.get("vocabulary") or {}
        cefr_dist = vocab_data.get("cefr_distribution", {}) if vocab_data else {}
        for level in ["A1", "A2", "B1", "B2", "C1", "C2"]:
            level_data = cefr_dist.get(level, {})
            if isinstance(level_data, dict):
                words = level_data.get("words", [])
                if isinstance(words, list):
                    vocab_overall[level]["words"].extend(words)
                    # Count will be calculated after deduplication below
    
    # Deduplicate vocab words and calculate percentages
    # Helper to get string from word (handles dict {"target": ..., "native": ...} or plain string)
    def word_to_str(w):
        if isinstance(w, dict):
            return w.get("target", str(w))
        return str(w) if w else ""
    
    # Deduplicate using string representation
    for level in vocab_overall:
        seen_words = set()
        unique_words = []
        for w in vocab_overall[level]["words"]:
            w_str = word_to_str(w)
            if w_str and w_str not in seen_words:
                seen_words.add(w_str)
                unique_words.append(w)
        vocab_overall[level]["words"] = unique_words
        vocab_overall[level]["count"] = len(unique_words)
    
    total_vocab_words = sum(vocab_overall[level]["count"] for level in vocab_overall)
    for level in vocab_overall:
        vocab_overall[level]["percentage"] = round((vocab_overall[level]["count"] / total_vocab_words * 100), 1) if total_vocab_words > 0 else 0
    
    feedback["wpm_per_turn"] = wpm_per_turn
    feedback["vocab_overall"] = vocab_overall
    
    return feedback
